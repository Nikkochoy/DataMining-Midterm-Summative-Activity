{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-29T14:24:34.503700Z",
     "start_time": "2025-06-29T14:24:34.496772Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. Load your dataset (assumes a CSV with a 'text' column)\n",
    "df = pd.read_csv('../data/merged_data.csv')\n",
    "texts = df['Tweet'].dropna().tolist()"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T14:24:35.636256Z",
     "start_time": "2025-06-29T14:24:34.514055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For demonstration, replace the above with your actual path.\n",
    "\n",
    "def preprocess(text, lemmatizer, stop_words):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\", \"\", text)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "base_stops = stopwords.words('english')\n",
    "custom_stops = ['climate', 'change', 'amp', 'rt', 'tweet']\n",
    "base_stops.extend(custom_stops)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess all tweets\n",
    "processed_texts = [preprocess(t, lemmatizer, base_stops) for t in texts]\n",
    "\n",
    "# 2. Vectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=10, max_features=1500, ngram_range=(1,2))\n",
    "dt_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# 3. Train LDA\n",
    "n_topics = 3  # at least 3 topics\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=32,\n",
    "                                learning_method='online',\n",
    "                                random_state=42,\n",
    "                                doc_topic_prior=0.1,   # lower alpha for more distinct topics\n",
    "                                topic_word_prior=0.01  # lower beta for sharper word distributions\n",
    "                            )\n",
    "lda.fit(dt_matrix)\n",
    "\n",
    "# 4. Display topics\n",
    "def display_topics(model, feature_names, no_top_words=3):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        # get feature indices sorted by weight (highest first)\n",
    "        sorted_indices = topic.argsort()[:-no_top_words*2 - 1:-1]\n",
    "\n",
    "        unique_terms = []\n",
    "        for i in sorted_indices:\n",
    "            term = feature_names[i]\n",
    "            if term not in unique_terms:\n",
    "                unique_terms.append(term)\n",
    "            if len(unique_terms) == no_top_words:\n",
    "                break\n",
    "\n",
    "        print(f\"Topic {idx + 1}: {' '.join(unique_terms)}\")\n",
    "\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, 3)\n",
    "\n",
    "# Replace comments and demo placeholders with your actual data loading and processed_texts list\n",
    "\n",
    "# Tips:\n",
    "# - Tune max_df/min_df to filter out very common or rare words.\n",
    "# - Experiment with n_components >= 3.\n",
    "# - Increase max_iter for better convergence.\n",
    "# - Consider using gensim for distributed or larger corpora."
   ],
   "id": "c472e45a615f7e73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: level sea rising\n",
      "Topic 2: crisis global environmental\n",
      "Topic 3: carbon footprint carbon footprint\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
